Database normalization is the process of structuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by British computer scientist Edgar F. Codd as part of his relational model. Normalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints. It is accomplished by applying some formal rules either by a process of synthesis (creating a new database design) or decomposition (improving an existing database design). A basic objective of the first normal form defined by Codd in 1970 was to permit data to be queried and manipulated using a "universal data sub-language" grounded in first-order logic. An example of such a language is SQL, though it is one that Codd regarded as seriously flawed. The objectives of normalisation beyond 1NF (first normal form) were stated by Codd as: To free the collection of relations from undesirable insertion, update and deletion dependencies. To reduce the need for restructuring the collection of relations, as new types of data are introduced, and thus increase the life span of application programs. To make the relational model more informative to users. To make the collection of relations neutral to the query statistics, where these statistics are liable to change as time goes by. — E.F. Codd, "Further Normalisation of the Data Base Relational Model" When an attempt is made to modify (update, insert into, or delete from) a relation, the following undesirable side effects may arise in relations that have not been sufficiently normalized: Insertion anomaly There are circumstances in which certain facts cannot be recorded at all. For example, each record in a "Faculty and Their Courses" relation might contain a Faculty ID, Faculty Name, Faculty Hire Date, and Course Code. Therefore, the details of any faculty member who teaches at least one course can be recorded, but a newly hired faculty member who has not yet been assigned to teach any courses cannot be recorded, except by setting the Course Code to null. Update anomaly The same information can be expressed on multiple rows; therefore updates to the relation may result in logical inconsistencies. For example, each record in an "Employees' Skills" relation might contain an Employee ID, Employee Address, and Skill; thus a change of address for a particular employee may need to be applied to multiple records (one for each skill). If the update is only partially successful – the employee's address is updated on some records but not others – then the relation is left in an inconsistent state. Specifically, the relation provides conflicting answers to the question of what this particular employee's address is. Deletion anomaly Under certain circumstances, the deletion of data representing certain facts necessitates the deletion of data representing completely different facts. The "Faculty and Their Courses" relation described in the previous example suffers from this type of anomaly, for if a faculty member temporarily ceases to be assigned to any courses, the last of the records on which that faculty member appears must be deleted, effectively also deleting the faculty member, unless the Course Code field is set to null. A fully normalized database allows its structure to be extended to accommodate new types of data without changing existing structure too much. As a result, applications interacting with the database are minimally affected. Normalized relations, and the relationship between one normalized relation and another, mirror real-world concepts and their interrelationships. Codd introduced the concept of normalization and what is now known as the first normal form (1NF) in 1970. Codd went on to define the second normal form (2NF) and third normal form (3NF) in 1971, and Codd and Raymond F. Boyce defined the Boyce–Codd normal form (BCNF) in 1974. Informally, a relational database relation is often described as "normalized" if it meets third normal form. Most 3NF relations are free of insertion, updation, and deletion anomalies. The normal forms (from least normalized to most normalized) are: UNF: Unnormalized form 1NF: First normal form 2NF: Second normal form 3NF: Third normal form EKNF: Elementary key normal form BCNF: Boyce–Codd normal form 4NF: Fourth normal form ETNF: Essential tuple normal form 5NF: Fifth normal form DKNF: Domain-key normal form 6NF: Sixth normal form Normalization is a database design technique, which is used to design a relational database table up to higher normal form. The process is progressive, and a higher level of database normalization cannot be achieved unless the previous levels have been satisfied. That means that, having data in unnormalized form (the least normalized) and aiming to achieve the highest level of normalization, the first step would be to ensure compliance to first normal form, the second step would be to ensure second normal form is satisfied, and so forth in order mentioned above, until the data conform to sixth normal form. However, it is worth noting that normal forms beyond 4NF are mainly of academic interest, as the problems they exist to solve rarely appear in practice. The data in the following example were intentionally designed to contradict most of the normal forms. In practice it is often possible to skip some of the normalization steps because the data is already normalized to some extent. Fixing a violation of one normal form also often fixes a violation of a higher normal form. In the example, one table has been chosen for normalization at each step, meaning that at the end, some tables might not be sufficiently normalized. Let a database table exist with the following structure: For this example it is assumed that each book has only one author. A table that conforms to the relational model has a primary key which uniquely identifies a row. Two books could have the same title, but an ISBN uniquely identifies a book, so it can be used as the primary key: In the first normal form each field contains a single value. A field may not contain a set of values or a nested record. Subject contains a set of subject values, meaning it does not comply. To solve the problem, the subjects are extracted into a separate Subject table: In Subject, ISBN is a foreign key: It refers to the primary key in Book, and makes the relationship between these two tables explicit. Instead of one table in unnormalized form, there are now two tables conforming to the 1NF. The Book table below has a composite key of {Title, Format} (indicated by the underlining), which will not satisfy 2NF if some subset of that key is a determinant. At this point in our design the key is not finalised as the primary key, so it is called a candidate key. Consider the following table: All of the attributes that are not part of the candidate key depend on Title, but only Price also depends on Format. To conform to 2NF and remove duplicates, every non-candidate-key attribute must depend on the whole candidate key, not just part of it. To normalize this table, make {Title} a (simple) candidate key (the primary key) so that every non-candidate-key attribute depends on the whole candidate key, and remove Price into a separate table so that its dependency on Format can be preserved: