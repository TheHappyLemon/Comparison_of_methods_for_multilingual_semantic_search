Regresijas analīzes galvenais uzdevums ir pētīt sakarības starp rezultatīvo pazīmi y un faktoriālo pazīmi x un novērtēt šīs sakarības funkciju. Regresijas analīze nosaka, pēc kādas likumsakarības mainās rezultatīvā pazīme, ja mainās faktoriālās pazīmes vērtība. Pilnīgi lineārajām funkcionālām sakarībām ir raksturīgs, ka katrai x i {\displaystyle x_{i}} vērtībai atbilst viena noteikta y i {\displaystyle y_{i}} vērtība, tātad attiecības ir pilnībā prognozējamas. Terminu regresija ir ieviesis Frānsiss Goltons. Savā rakstā viņš atzīmēja, ka, lai gan garākiem vecākiem ir tendence piedzimt garākiem bērniem un īsākiem otrādi, tomēr abiem vecāku veidiem bērnu augumam ir tendence virzīties jeb regresēt uz vidējo augumu visā populācijā. Galtona likumu apstiprināja Karls Pīrsons, kas savāca vairāk nekā tūkstoti ierakstu par cilvēku augumiem. Tātad ar parastās viena faktora regresijas palīdzību ir iespējams prognozēt dēla augumu, ja ir zināms viņa tēva augums. Lineāro regresijas modeli vispārīgā veidā var pierakstīt šādi: Y = α ^ + β ^ X . {\displaystyle Y={\hat {\alpha }}+{\hat {\beta }}X.} kur Y {\displaystyle Y}  — rezultatīvā pazīme, X {\displaystyle X}  — faktoriālā pazīme, α ^ , β ^ {\displaystyle {\hat {\alpha }},{\hat {\beta }}}  — modeļa parametri, kuri jāaprēķina. Plašākos pētījumos, kad vienā darbā jāaplūko vairāki modeļi ar dažādām rezultatīvām un faktoriālām pazīmēm, tās kodē ar skaitļiem. Sakarību starp divām pazīmēm sauc par korelatīvu, ja faktoriālās pazīmes izmaiņas ir saistītas ar rezultatīvās pazīmes vidējo vērtību izmaiņām. Modeli statistikā un ekonometrijā sauc par vienkāršu (pāru) lineāru regresijas vienādojumu, tā koeficientu β ^ {\displaystyle {\hat {\beta }}} par regresijas koeficientu, bet α ^ {\displaystyle {\hat {\alpha }}}  — par vienādojuma brīvo locekli. Lai modelis kļūtu konkrēts un atspoguļotu interesējošās sakarības, ir jānosaka, izmantojot statistikas datus, parametru α ^ {\displaystyle {\hat {\alpha }}} un β ^ {\displaystyle {\hat {\beta }}} vērtības. To, var izdarīt ar vairākām metodēm. Statistikā un ekonometrijā visplašāko pielietojumu ir guvusi mazāko kvadrātu metode. Saskaņā ar to, α ^ {\displaystyle {\hat {\alpha }}} un β ^ {\displaystyle {\hat {\beta }}} jāizvēlas tā, lai noviržu kvadrātu summa faktiski novērotām un ar modeli aprēķinātām rezultatīvās pazīmes vērtībām būtu minimāla. Matemātiskajā statistikā pierāda, ka šīm prasībām atbilst taisne, kuras parametri a un b ir šādi: β ^ = n ∑ x i y i − ∑ x i ∑ y i n ∑ x i 2 − ( ∑ x i ) 2 , α ^ = y ¯ − β ^ x ¯   . {\displaystyle {\hat {\beta }}={\frac {n\sum {x_{i}y_{i}}-\sum {x_{i}}\sum {y_{i}}}{n\sum {x_{i}^{2}}-(\sum {x_{i}})^{2}}},\quad {\hat {\alpha }}={\overline {y}}-{\hat {\beta }}\,{\overline {x}}\ .} Korelācija Lineāra funkcija Mazāko kvadrātu metode Encyclopedia of Mathematics ieraksts