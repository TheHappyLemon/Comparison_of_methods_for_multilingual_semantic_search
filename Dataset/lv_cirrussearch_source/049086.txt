LSTM — rekurentā mākslīgā neironu tīkla (RNN) paveids “Long Short-Term Memory”, kas tiek izmantots dziļās mācīšanās jomā. Atšķirībā no standarta neironu tīkliem, LSTM ir atgriezeniskās saites savienojumi. Ar LSTM var aprēķināt visu, ko var izmantot Tjūringa mašīna. Tam nevar būt viens datu punkts (piem., attēls vai video). LSTM ir piemērots tādiem uzdevumiem kā saistītā rokraksta atpazīšana un runas atpazīšana. Bloomberg Business Week rakstīja: "Šīs īpašības neapšaubāmi padara LSTM par vislielāko komerciālo AI sasniegumu, ko izmanto visur, sākot no slimību prognozēšanas līdz mūzikas veidošanai." Bieži sastopama LSTM vienība sastāv no šūnas, ievada vārtiem, izvada vārtiem un aizmiršanas vārtiem. Šūna atceras vērtības patvaļīgos laika intervālos, un trīs vārti regulē informācijas plūsmu uz un no šūnas. LSTM tika izstrādāts, lai risinātu pārslodzes un izzūdošo gradientu problēmas, kas var rasties, apmācot tradicionālos RNN. Relatīvā nejutība pret tukšiem datiem ir LSTM priekšrocība pār RNN, slēptajiem Markova modeļiem un citām secību mācīšanās metodēm daudzās lietojumprogrammatūrās. 1997. gadā Zeps Hohreiters un Jirgens Šmīdhūbers ierosināja LSTM. Ieviešot Constant Error Carousel (CEC) vienības, LSTM nodarbojas ar pārslodzes un izzūdošām gradienta problēmām. LSTM bloka sākotnējā versija ietvēra šūnas, ievada un izvada vārtus. 1999. gadā Felikss Gers, viņa padomnieks Jirgens Šmīdhūbers un Freds Cummins LSTM arhitektūrā ieviesa aizmiršanas vārtus (sauktus arī par “turēšanas vārtiem”), ļaujot LSTM atjaunot savu stāvokli. 2000. gadā Gers, Šmīdhūbers un Cummins pievienoja arhitektūrā savienojumus no šūnas uz vārtiem. Turklāt izejas aktivizēšanas funkcija tika izlaista. Citu panākumu vidū LSTM sasniedza rekorda rezultātus dabiskās valodas teksta saspiešanā, saistītā rokraksta atpazīšanā un uzvarēja ICDAR rokraksta sacensībās (2009). Kopš 2016. gada vadošie tehnoloģiju uzņēmumi, tostarp Google, Apple un Microsoft, izmantoja LSTM kā pamatelementu jaunos produktos. Google izmantoja LSTM runas atpazīšanai viedtālrunī, viedajam palīgam Allo un Google tulkotājam. Apple izmanto LSTM "Quicktype" funkcijai iPhone un Siri. Amazon izmanto LSTM iekš Amazon Alexa. 2017. gadā Facebook katru dienu veica aptuveni 4,5 miljardus automātisko tulkojumu, izmantojot LSTM tīklus. 2017. gadā Mičiganas Universitāte, IBM pētniecība un Kornela Universitāte pētnieki publicēja pētījumu “Knowledge Discovery and Data Mining” (KDD) konferencē. Viņu pētījums apraksta jaunu neironu tīklu, kas noteiktus datu kopumus apstrādā labāk nekā plaši izmantotais LSTM tīkls. Teorētiski, klasiskais RNN var izsekot patvaļīgām ilgtermiņa atkarībām. RNN problēma ir skaitļošanā — RNN apmācības laikā, izmantojot back-propagation, gradienti var "izzust" (tie var būt nulle) vai “pārslogoties” (tie ir tendēti uz bezgalību), jo procesā iesaistītie aprēķini izmanto galīgās precizitātes skaitļus. Izmantojot LSTM vienības, RNN daļēji atrisina gradienta izzušanas problēmu, jo LSTM vienības ļauj gradientiem palikt nemainītiem. Tomēr LSTM tīkli joprojām var ciest no gradienta pārslodzes problēmas. Ir vairākas LSTM vienību arhitektūras. Kopēja arhitektūra sastāv no šūnas (LSTM vienības atmiņas daļas) un trīs informācijas plūsmas "regulatoriem" (vārtiem) — ievada vārtiem, izvada vārtiem un aizmiršanas vārtiem. Dažām LSTM vienības variācijām nav vieni vai vairāki no šiem vārtiem vai varbūt ir citi vārti. Piemēram, GRU nav izvada vārtu. Šūna ir atbildīga par atkarības saglabāšanu starp ievades secībā esošajiem elementiem. Ievada vārti kontrolē, cik daudz jauno vērtību ieplūst šūnā, aizmiršanas vārti kontrolē, cik lielā mērā vērtība paliek šūnā, un izvada vārti kontrolē, kāda daļa šūnas vērtības tiek izmantota izvada aprēķināšanai LSTM vienībai. LSTM vārtu aktivizēšanas funkcija bieži ir loģistikas funkcija. Ir savienojumi uz un no LSTM vārtiem, no kuriem daži atkārtojas. Šo savienojumu svars, kas jāapgūst treniņa laikā, nosaka, kā darbojas vārti. Turpmākajos vienādojumos mazie burti ir vektori. Matricas W q {\displaystyle W_{q}} un U q {\displaystyle U_{q}} satur ievada un atkārtojumu svarojuma saistību, kur indekss q {\displaystyle _{q}} var būt gan ievada vārti i {\displaystyle i} , izvada vārti o {\displaystyle o} , aizmiršanas vārti f {\displaystyle f} vai atmiņas šūna c {\displaystyle c} , atkarībā no tā, kas tiek aprēķināts. Šajā sadaļā tiek izmants "vektora apzīmējums". Piemēram, c t ∈ R h {\displaystyle c_{t}\in \mathbb {R} ^{h}} ir nevis viena LSTM vienības šūna, bet h {\displaystyle h} LSTM vienību šūnas. Kompaktas vienādojumu formas padotajām LSTM vienībām ar aizmiršanas vārtiem: f t = σ g ( W f x t + U f h t − 1 + b f ) i t = σ g ( W i x t + U i h t − 1 + b i ) o t = σ g ( W o x t + U o h t − 1 + b o ) c t = f t ∘ c t − 1 + i t ∘ σ c ( W c x t + U c h t − 1 + b c ) h t = o t ∘ σ h ( c t ) {\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}} kur sākotnējās vērtības ir c 0 = 0 {\displaystyle c_{0}=0} un h 0 = 0 {\displaystyle h_{0}=0} un operators ∘ {\displaystyle \circ } apzīmē Hadamard produktu (element-gudrs produkts). Apakšindekss t {\displaystyle t} apzīmē laika soli. x t ∈ R d {\displaystyle x_{t}\in \mathbb {R} ^{d}} : LSTM vienības ievades vektors f t ∈ R h {\displaystyle f_{t}\in \mathbb {R} ^{h}} : aizmiršanas vārtu aktivizācijas vektors i t ∈ R h {\displaystyle i_{t}\in \mathbb {R} ^{h}} : ievades vārtu aktivizācijas vektors o t ∈ R h {\displaystyle o_{t}\in \mathbb {R} ^{h}} : izvades vārtu aktivizācijas vektors h t ∈ R h {\displaystyle h_{t}\in \mathbb {R} ^{h}} : slēptā stāvokļa vektors, zināms arī kā LSTM vienības izejas vektors c t ∈ R h {\displaystyle c_{t}\in \mathbb {R} ^{h}} : šūnas stāvokļa vektors W ∈ R h × d {\displaystyle W\in \mathbb {R} ^{h\times d}} , U ∈ R h × h {\displaystyle U\in \mathbb {R} ^{h\times h}} un b ∈ R h {\displaystyle b\in \mathbb {R} ^{h}} : svara matricas un novirzes vektora parametri, kas jāapgūst apmācības laikā mainīgie d {\displaystyle d} un h {\displaystyle h} attiecas uz ievades funkciju skaitu un slēpo vienības skaitu. σ g {\displaystyle \sigma _{g}} : sigmoid funkcija. σ c {\displaystyle \sigma _{c}} : hyperbolic tangent funkcija. σ h {\displaystyle \sigma _{h}} : hyperbolic tangent funkcija vai kā LSTM raksts raksta, σ h ( x ) = x {\displaystyle \sigma _{h}(x)=x} . Peephole savienojumi ļauj vārtiem piekļūt constant error carousel (CEC), kura aktivizācija ir šūnas stāvoklis. h t − 1 {\displaystyle h_{t-1}} netiek izmantots, tā vietā vairums vietās izmanto c t − 1 {\displaystyle c_{t-1}} . f t = σ g ( W f x t + U f c t − 1 + b f ) i t = σ g ( W i x t + U i c t − 1 + b i ) o t = σ g ( W o x t + U o c t − 1 + b o ) c t = f t ∘ c t − 1 + i t ∘ σ c ( W c x t + U c c t − 1 + b c ) h t = o t ∘ σ h ( c t ) {\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}x_{t}+U_{i}c_{t-1}+b_{i})\\o_{t}&=\sigma _{g}(W_{o}x_{t}+U_{o}c_{t-1}+b_{o})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}x_{t}+U_{c}c_{t-1}+b_{c})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}} Peephole konvolūcijas LSTM. Simbols ∗ {\displaystyle *} apzīmē konvulūcijas operatoru. f t = σ g ( W f ∗ x t + U f ∗ h t − 1 + V f ∘ c t − 1 + b f ) i t = σ g ( W i ∗ x t + U i ∗ h t − 1 + V i ∘ c t − 1 + b i ) c t = f t ∘ c t − 1 + i t ∘ σ c ( W c ∗ x t + U c ∗ h t − 1 + b c ) o t = σ g ( W o ∗ x t + U o ∗ h t − 1 + V o ∘ c t + b o ) h t = o t ∘ σ h ( c t ) {\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}*x_{t}+U_{f}*h_{t-1}+V_{f}\circ c_{t-1}+b_{f})\\i_{t}&=\sigma _{g}(W_{i}*x_{t}+U_{i}*h_{t-1}+V_{i}\circ c_{t-1}+b_{i})\\c_{t}&=f_{t}\circ c_{t-1}+i_{t}\circ \sigma _{c}(W_{c}*x_{t}+U_{c}*h_{t-1}+b_{c})\\o_{t}&=\sigma _{g}(W_{o}*x_{t}+U_{o}*h_{t-1}+V_{o}\circ c_{t}+b_{o})\\h_{t}&=o_{t}\circ \sigma _{h}(c_{t})\end{aligned}}} Sepp Hochreiter; Jürgen Schmidhuber (1997). "Long Short-term Memory". Neural Computation 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. PMID 9377276. Hava T. Siegelmann, Eduardo D. Sontag. On the Computational Power of Neural Nets. COLT '92. ACM, 1992. 440–449. lpp. ISBN 978-0-89791-497-0. doi:10.1145/130385.130432. Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (2009). "A Novel Connectionist System for Improved Unconstrained Handwriting Recognition". IEEE Transactions on Pattern Analysis and Machine Intelligence 31 (5): 855–868. doi:10.1109/tpami.2008.137. PMID 19299860. Hasim Sak, Andrew Senior, Francoise Beaufays. «Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling», 2014. Arhivēts no oriģināla, laiks: 2018. gada 24. aprīlī. Skatīts: 2019. gada 2. jūnijā. Ashlee Vance. «Quote: These powers make LSTM arguably the most commercial AI achievement, used for everything from predicting diseases to composing music.». Bloomberg Business Week, 2018. Klaus Greff; Rupesh Kumar Srivastava; Jan Koutník; Bas R. Steunebrink; Jürgen Schmidhuber (2015). "LSTM: A Search Space Odyssey". IEEE Transactions on Neural Networks and Learning Systems 28 (10): 2222–2232. arXiv:1503.04069. doi:10.1109/TNNLS.2016.2582924. PMID 27411231. Felix Gers; Jürgen Schmidhuber; Fred Cummins (1999). "Learning to Forget: Continual Prediction with LSTM". Proc. ICANN'99, IEE, London: 850–855. Felix A. Gers; Jürgen Schmidhuber; Fred Cummins (2000). "Learning to Forget: Continual Prediction with LSTM". Neural Computation 12 (10): 2451–2471. doi:10.1162/089976600300015015. Matt Mahoney. «The Large Text Compression Benchmark», 2019. Graves, A.; Liwicki, M.; Fernández, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (2009. gada maijs). "A Novel Connectionist System for Unconstrained Handwriting Recognition". IEEE Transactions on Pattern Analysis and Machine Intelligence 31 (5): 855–868. doi:10.1109/tpami.2008.137. ISSN 0162-8828. PMID 19299860. Metz, Cade (2016. gada jūnijs). "Apple is bringing the AI revolution to your iphone.". WIRED. Françoise Beaufays. «The neural networks behind Google Voice transcription». Research Blog, 11.08.2015. Haşim Haşim Sak, Andrew Senior, Kanishka Rao, Françoise Beaufays, Johan Schalkwyk. «Google voice search: faster and more accurate». Research Blog, 24.09.2015. Pranav Khaitan. «Chat Smarter with Allo». Research Blog, 18.05.2016. Metz, Cade (27.09.2016). "An Infusion of AI Makes Google Translate More Powerful Than Ever | WIRED". Wired. Amir Efrati. «Apple's Machines Can Learn Too». The Information. Steve Ranger. «iPhone, AI and big data: Here's how Apple plans to protect your privacy | ZDNet», 14.06.2016. Chris Smith. «iOS 10: Siri now works in third-party apps, comes with extra AI features». BGR. Werner Vogels. «Bringing the Magic of Amazon AI and Alexa to Apps on AWS. - All Things Distributed». www.allthingsdistributed.com. Thuy Ong. «Facebook's translations are now powered completely by AI». www.allthingsdistributed.com. Inci Baytas ;Cao Xiao ;Xi Zhang ;Fei Wang ;Anil Jain;Jiayu Zhou. «Patient Subtyping via Time-Aware LSTM Networks», 2017. gada augusts. Inci Baytas ;Cao Xiao ;Xi Zhang ;Fei Wang ;Anil Jain;Jiayu Zhou. «Patient Subtyping via Time-Aware LSTM Networks». Kdd.org. «Why can RNNs with LSTM units also suffer from "exploding gradients"?». Cross Validated. Gers, F. A.; Schmidhuber, J. (2001). "LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages". IEEE Transactions on Neural Networks 12 (6): 1333–1340. doi:10.1109/72.963769. PMID 18249962. Gers, F.; Schraudolph, N.; Schmidhuber, J. (2002). "Learning precise timing with LSTM recurrent networks". Journal of Machine Learning Research 3: 115–143. Gers, F. A.; Schmidhuber, E. (November 2001). "LSTM recurrent networks learn simple context-free and context-sensitive languages". IEEE Transactions on Neural Networks 12 (6): 1333–1340. doi:10.1109/72.963769. ISSN 1045-9227. PMID 18249962. Xingjian Shi; Zhourong Chen; Hao Wang; Dit-Yan Yeung; Wai-kin Wong; Wang-chun Woo (2015). "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting". Proceedings of the 28th International Conference on Neural Information Processing Systems: 802–810. arXiv:1506.04214. Bibcode 2015arXiv150604214S.