
from transformers import AutoTokenizer, AutoModel
from utils import *
import faiss
from constants import path_res, path_setup
import os
from exceptions import EmbeddingsError
import json
from datetime import datetime

k = 5

def get_pages_text_count(dataset_path : str, limit : int = -1):
    processed = 0
    if limit != -1:
        return limit

    for filename in os.listdir(dataset_path):
        full_path = os.path.join(dataset_path, filename)
        if os.path.isfile(full_path):
            processed = processed + 1
    return processed

def get_pages_texts(dataset_path : str, start_at : int = 0, stop_at : int = -1, max_limit : int = -1):
    texts = []
    processed = 0
    
    # So order is always the same!
    filenames = sorted(os.listdir(dataset_path))
    if stop_at == -1:
        stop_at = len(filenames)

    for filename in filenames:
        full_path = os.path.join(dataset_path, filename)
        if os.path.isfile(full_path):
            processed = processed + 1
            if processed < start_at or processed > stop_at:
                continue
            with open(full_path, 'r', encoding='utf-8') as file:
                text = file.read()
                texts.append(text)
                if max_limit != -1 and len(texts) >= max_limit:
                    break
    return texts

def get_np_array_zero_rows(np_array):
    return np.where(~np.any(np_array, axis=1))[0]

def build_index_from_hdf(file_path : str, dataset : str):
    with h5py.File(file_path, 'r') as file:
        if not dataset in file:
            raise ValueError(f"Dataset {dataset} not in file")
        # np.any - non zero rows, ~ - invert, np.where - return array of indexes with True values
        zero_rows_indexes = get_np_array_zero_rows(file[dataset])
        if len(zero_rows_indexes) != 0:
            raise EmbeddingsError(f"Some embeddings in dataset '{dataset}' are not calculated", zero_rows_indexes[0])
        index = faiss.IndexHNSWFlat(file[dataset].shape[1], 64)
        index.add(file[dataset])
        return index

def get_embedding_from_hdf(file_path : str, dataset : str, index : int):
    with h5py.File(file_path, 'r') as file:
        if not dataset in file:
            raise ValueError(f"Dataset {dataset} not in file")
        return file[dataset][index]
        
def get_dict_from_json(file : str):
    try:
        with open(file, 'r') as f:
            res = json.load(f)
            return res
    except FileNotFoundError:
        print(file)
        return None

def create_datasets_if_not_exist(file_name : str, names : dict, wiki_types : list, embedding_langs : list):

    ###### FILE STRUCTURE ######
    #    Model_name            #
    #    ├── title             #
    #    │   ├── en            #
    #    │   └── lv            #
    #    ├── open              #
    #    │   ├── en            #
    #    │   └── lv            #
    #    └── source            #
    #    │   ├── en            #
    #    │   └── lv            #
    #    ...                   #
    ############################

    cache = {}

    with h5py.File(file_name, 'a') as file:
        for model_name in names:
            tokenizer = AutoTokenizer.from_pretrained(names[model_name])
            model = AutoModel.from_pretrained(names[model_name])

            if not model_name in file:
                test_case = get_embedding("Test", tokenizer, model)
                model_group = file.create_group(model_name)
                for wiki_type in wiki_types:
                    type_group = model_group.create_group(wiki_type)
                    for lang in embedding_langs:
                        
                        path_to_data = path_res + lang + '_' + wiki_type
                        if not path_to_data in cache:
                            cache[path_to_data] = get_pages_text_count(path_res + lang + '_' + wiki_type)
                        
                        #type_group.create_dataset(lang, shape=(cache[path_to_data], test_case.shape[1]), compression='gzip', chunks=(5, 768))
                        type_group.create_dataset(lang, shape=(cache[path_to_data], test_case.shape[1]), chunks=(25, 768))

def fill_datasets_if_empty(file_name : str, names : dict, wiki_types : list, embedding_langs : list):

    batch_size = 10
    start = datetime.now()

    with h5py.File(file_name, 'a') as file:
        for model_name in names:

            tokenizer = AutoTokenizer.from_pretrained(names[model_name])
            model = AutoModel.from_pretrained(names[model_name])

            group_name = file[model_name]
            for wiki_type in wiki_types:
                type_group = group_name[wiki_type]
                for lang in embedding_langs:
                    dataset = type_group[lang]
                    zero_rows_index = get_np_array_zero_rows(dataset)
                    # Start recalculating from first zero row. Rows are calculated one by one, so rows after also must be zeros
                    if len(zero_rows_index) != 0:
                        batch_offset = zero_rows_index[0]
                        texts = get_pages_texts(path_res + lang + '_' + wiki_type, start_at=batch_offset, max_limit=52)

                        for i in range(0, len(texts), batch_size):
                            batch_texts = texts[i:i + batch_size]
                            encoded_input = tokenizer(batch_texts, padding='max_length', truncation=True, return_tensors="pt")
                            
                            # torch.no_grad - we are NOT training. It does not store gradients during forward pass
                            with torch.no_grad():
                                model_output = model(**encoded_input)
                            embeddings = cls_pooling(model_output).numpy()

                            for j in range(len(embeddings)):
                                dataset[i + j + batch_offset] = embeddings[j]
                            print(f"Flushed {len(batch_texts)} units")
                            file.flush()
    end = datetime.now()
    print(f'Been working {end - start}')

if __name__ == '__main__':
    # definitions
    hdf5_file = path_res + f'\\embeddings.hdf5'

    # Read setup files and extract which models we want, what kind of wikipedia pages and what language embeddings to generate
    models = get_dict_from_json(path_setup + "models.json")
    wiki_types = get_dict_from_json(path_setup + "wiki_types.json")
    embedding_langs = get_dict_from_json(path_setup + "embedding_types.json")
    if (models is None) or (wiki_types is None) or (embedding_langs is None):
        print(f"Some setup files are not on disk at '{path_setup}'!")
        exit()
    wiki_types = [key for key, value in wiki_types.items() if value is True]
    embedding_langs = [key for key, value in embedding_langs.items() if value is True]
    
    create_datasets_if_not_exist(hdf5_file, models, wiki_types, embedding_langs)
    fill_datasets_if_empty(hdf5_file, models, wiki_types, embedding_langs)
    
    exit()
    index = None
    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')
    model = AutoModel.from_pretrained('bert-base-multilingual-cased')
    en_embeddings_name = "en_embeddings"
    lv_embeddings_name = "lv_embeddings"
    
    try:
        index = build_index_from_hdf(hdf5_file, en_embeddings_name)
    except FileNotFoundError as e1:
        # Generate file with english embeddings from scratch. Will take a lot of time...
        en_texts = get_pages_texts(dataset_path=path_res + "en_cirrussearch_title", max_limit=30)
        generate_embeddings(en_texts,  tokenizer, model, en_embeddings_name)
        #index = build_index_from_hdf(hdf5_file, en_embeddings_name)
    except EmbeddingsError as e2:
        # Some part of embeddings has already been calculated, but some not -> continue
        en_texts = get_pages_texts(dataset_path=path_res + "en_cirrussearch_title", start_at=e2.indexes, max_limit=6)
        generate_embeddings(en_texts, tokenizer, model, en_embeddings_name, batch_offset=e2.indexes)
        #index = build_index_from_hdf(hdf5_file, en_embeddings_name)
    except Exception as e3:
        print("Error:", repr(e3))
        exit()

    with h5py.File(hdf5_file, 'r') as file:
        lv_embeddings_exist = lv_embeddings_name in file
        if lv_embeddings_exist:
            zero_rows_index = get_np_array_zero_rows(file[lv_embeddings_name])[0]
    
    if not lv_embeddings_exist:
        # calculate latvian embeddings if there are none
        lv_texts = get_pages_texts(dataset_path=path_res + "lv_cirrussearch_title", max_limit=5)
        generate_embeddings(lv_texts,  tokenizer, model, lv_embeddings_name)
    else:
        # Some part of embeddings has already been calculated, but some not -> continue
        lv_texts = get_pages_texts(dataset_path=path_res + "lv_cirrussearch_title", start_at=zero_rows_index, max_limit=6)
        generate_embeddings(lv_texts,  tokenizer, model, lv_embeddings_name, batch_offset=zero_rows_index)

    text_count = get_pages_text_count(path_res + "lv_cirrussearch_title", 5)
    for i in range(text_count):
        e = get_embedding_from_hdf(hdf5_file, lv_embeddings_name, i)
        print(type(e))